##llamar packages
require(pacman)
p_load(tidyverse, rvest, tibble)
## Scraping de los datos en chunk
url_geih18 <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
browseURL(url_geih18)
## leer el html de la página del profe
html_t1 <- read_html(url_geih18)
class(html_t1)
## topar el Xpath para encontar las url de los chuns
html_t1 %>% html_nodes(xpath = 'html/body/div/div/div[2]/ul/li[1]/a')
## Extraer los elementos del nodo a correspndientes a los links
elements <- html_t1 %>% html_elements("a")
elements
#Almacenar los links en un objeto
refs <- elements %>% html_attr("href")
refs <- refs[6:15]
#Referenciar los nombres de los chunk y enlistar las url obtenidad para cada chunk en un tibble
nom_chunk <- elements %>% html_text()
nom_chunk <- nom_chunk[6:15]
db_url<-tibble(nom_chunk,url=paste0(url_geih18,refs))
require(pacman)
p_load(tidyverse, rvest, tibble)
browseURL(url_geih18)
## leer el html de la página del profe
html_t1 <- read_html(url_geih18)
class(html_t1)
## topar el Xpath para encontar las url de los chuns
html_t1 %>% html_nodes(xpath = 'html/body/div/div/div[2]/ul/li[1]/a')
## Extraer los elementos del nodo a correspndientes a los links
elements <- html_t1 %>% html_elements("a")
elements
#Almacenar los links en un objeto
refs <- elements %>% html_attr("href")
refs <- refs[6:15]
#Referenciar los nombres de los chunk y enlistar las url obtenidad para cada chunk en un tibble
nom_chunk <- elements %>% html_text()
nom_chunk <- nom_chunk[6:15]
require(pacman)
p_load(tidyverse, rvest, tibble)
url_geih18 <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
browseURL(url_geih18)
## leer el html de la página del profe
html_t1 <- read_html(url_geih18)
class(html_t1)
html_t1 %>% html_nodes(xpath = 'html/body/div/div/div[2]/ul/li[1]/a')
elements <- html_t1 %>% html_elements("a")
elements
>% html_attr("href")
refs <- refs[6:15]
refs <- elements %>% html_attr("href")
refs <- refs[6:15]
refs
nom_chunk <- elements %>% html_text()
nom_chunk <- nom_chunk[6:15]
db_url<-tibble(nom_chunk,url=paste0(url_geih18,refs))
## Revisar el tibble de urls
db_url %>% head()
db_url$url[1]
n <- nrow(db_url)
n <- nrow(db_url)
n
db <- data.frame()
for (i in 1:n){
urls <- db_url$url[i] %>% read_html() %>% html_elements("div") %>% html_attr("w3-include-html") #se obtine la parte de url verdadera
urls <- urls[8] #Exploración del elemento 8 donde esta la url verdadera
dc<-tibble(nom_chunk,urls=paste0(url_geih18,urls)) #unir la url principal con la parte de url verdadera
dbc <- dc$urls[i] %>% read_html() %>% html_nodes("table") %>% html_table() %>% as.data.frame() # extracción de la tabla y conversión en DataFrame
db <- rbind(db, dbc) # append de las bases
cat("chunk OK:", i, ", ")
}
saveRDS(db,file=".../Data/GEIH_2018.rds")
setwd("~/Desktop/Problem_set_1_WS_JSV/Scrips")
saveRDS(db,file=".../Data/GEIH_2018.rds")
saveRDS(db,file="~/Data/GEIH_2018.rds")
saveRDS(db,file="Poblem_set_1_WS_JSV/Data/GEIH_2018.rds")
head(db)
View(db)
db_1 <- dB
db_1 <- db
p_load(rio,skimr,caret)
skim(db)
db_1[db_1$age >= 18, ]
db_1 <- db_1[db_1$age >= 18, ]
db_1 <- db_1[db_1$age >= 18 & db_1$ocu == 1, ]
db_1 <- table(dominio)
table(dominio)
summary(db_1$domino)
table(db_1$dominio)
db_1 <- db_1[db_1$age >= 18 & db_1$ocu == 1, ]
table(db_1$p6240)
